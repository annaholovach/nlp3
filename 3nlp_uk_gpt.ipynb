{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nagPvyCF7q_r",
    "outputId": "34214953-3def-4fd7-b65c-edd715b820fe"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate peft transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9P3ROQ4iq9LB",
    "outputId": "5dd1a3fc-ad47-4294-9458-ce87be96c202"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import zipfile\n",
    "\n",
    "zip_path = '/content/drive/MyDrive/ukr_lit.zip'\n",
    "extract_path = '/content/data'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbSiXDyLti2Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "content_path = \"/content/data/ukr_lit/texts/1\"\n",
    "all_text = \"\"\n",
    "\n",
    "# Сортуємо, якщо потрібен порядок\n",
    "for filename in sorted(os.listdir(content_path)):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(content_path, filename), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            # print(f\"content {content}\")\n",
    "            all_text += content + \"\\n\\n<|sep|>\\n\\n\"\n",
    "\n",
    "# with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#     out_file.write(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHmlpnW2zmw-",
    "outputId": "d30e38a4-bec3-4d27-bacd-03006b4f700a"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFzo6D1QzafZ",
    "outputId": "39c00f72-5e05-4927-a819-0c51f2db1cd0"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import re\n",
    "\n",
    "samples = all_text.split(\"<|sep|>\")\n",
    "\n",
    "# Очищаємо зайві пробіли\n",
    "samples = [\n",
    "    re.sub(r'\\n+', ' ', re.sub(r'\\n+Тарас Шевченко\\n+', ' ', s))  # Спочатку прибираємо \"Тарас Шевченко\", потім заміняємо всі переноси на пробіл\n",
    "    for s in samples\n",
    "]\n",
    "\n",
    "print(len(samples))\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": samples\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRTi0mWcA6jb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "train_dataset, val_dataset = shuffled_dataset.train_test_split(test_size=0.1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bc5IbDdA0LVQ",
    "outputId": "56ba392a-2dbf-455a-91d3-21ed3a5112d2"
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da17FS1h5Qik",
    "outputId": "78c3580e-178c-43ec-e20b-be457a322745"
   },
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"malteos/gpt2-uk\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"malteos/gpt2-uk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "4a1987eb971843b59ee4a0ba0a41944e",
      "ac7c60cd6dc94f40a1edd5515ae606c6",
      "dfb3d403a8a642de87f3fc4e21f3fd95",
      "84a2ddb9073147a7a039ce495131dc3f",
      "a0e9905ad76645c7bc4b467acf48d777",
      "31a6ca54a9bf413fa8bcc9360c5feb85",
      "72b07c1af9fb4aa9a4870eea30c1c559",
      "53bc73a016ff4005ab02965b25e98651",
      "25ce67b43ef747a6939370eb2a0b5605",
      "d28824545a99417ca1531b6f097938d2",
      "1c58ec91c3404901961a89e48d71fd08",
      "7b2e9988f4f249c28eea4c7e65e59479",
      "601ee4f3007a46fdb821bbc7bcc40edf",
      "eea4cbe0ab0644eea9b07c8b35083728",
      "33a99481fd8346b086bfd8ef6d01e313",
      "6b55a77ef74b4c35b4b6de737313446d",
      "1bb421da4cd74d8194856f88f3c83a08",
      "568a7d7b20e3494aa8117bcdec815b52",
      "b7273348e49949719ef57a97df0da512",
      "b0c0e1604e4440d4a81e56beb936be72",
      "99ec9c14ae6245f9a50b81b93665cbd4",
      "4e0c6592f6c449b6956fa9998176c89a"
     ]
    },
    "id": "4G98HHwm8mnD",
    "outputId": "ed9237d0-47e7-4499-d796-8deb0900cdd0"
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512, add_special_tokens=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8GO9dW287Xq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import torch\n",
    "\n",
    "class CustomDataset(TorchDataset):\n",
    "    def __init__(self, tokenizer, dataset):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        input_ids = torch.tensor(item['input_ids'])\n",
    "        attention_mask = torch.tensor(item['attention_mask'])\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "# Створюємо датасет PyTorch\n",
    "train_dataset = CustomDataset(tokenizer, tokenized_train_dataset)\n",
    "val_dataset = CustomDataset(tokenizer, tokenized_val_dataset)\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "# Створюємо DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNjT6SZXADgM"
   },
   "source": [
    "# Новий розділ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Zbq7n5M_GQr",
    "outputId": "997d33d4-7a3a-4eb3-f2da-ab941a0d7879"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(model, val_dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()  # Перемикаємо модель на режим оцінки\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Вимикаємо обчислення градієнтів\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Прямий прохід\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Обчислюємо Perplexity\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    perplexity = math.exp(avg_loss)  # Експоненціємо середні втрати\n",
    "    return perplexity\n",
    "\n",
    "# Приклад використання\n",
    "perplexity = calculate_perplexity(model, val_dataloader,  'cuda')\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7_YQlBu_9EK",
    "outputId": "2162b0ba-adea-479c-f22a-4bb7a31ad699"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu_score(model, dataloader, device):\n",
    "    model.eval()  # Перемикаємо модель на режим оцінки\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():  # Вимикаємо обчислення градієнтів\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Генерація тексту\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50)\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Додаємо еталонні та згенеровані тексти\n",
    "            reference_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            references.append([reference_text])  # Зберігаємо як список рядків\n",
    "            hypotheses.append(generated_text)  # Зберігаємо як рядок\n",
    "\n",
    "    # Обчислюємо BLEU score для всього набору даних\n",
    "    smoothing_function = SmoothingFunction().method4  # Можна змінити метод згладжування\n",
    "    bleu_score = sentence_bleu(references, hypotheses, smoothing_function=smoothing_function)\n",
    "    return bleu_score\n",
    "\n",
    "# Приклад використання\n",
    "bleu_score = calculate_bleu_score(model, val_dataloader, 'cuda')\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxO6u4Q_AGez"
   },
   "source": [
    "# Новий розділ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAKwXxuD-pVA",
    "outputId": "2f0e1006-dce9-4de7-f08f-fed468b4eeff"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "device = \"cuda\"\n",
    "num_epochs = 3\n",
    "\n",
    "def evaluate_model(model, val_dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass on the validation set\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Print the average validation loss\n",
    "    return val_loss / len(val_dataloader)\n",
    "\n",
    "def train_model(model, train_dataloader, optimizer, num_epochs, device):\n",
    "    model.to(device)  # Move model to the appropriate device (GPU or CPU)\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0  # To track loss for the entire epoch\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()  # Zero gradients before backward pass\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate loss for the epoch\n",
    "\n",
    "        # Print the average loss for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Average Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "\n",
    "        val_loss = evaluate_model(model, val_dataloader, device)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Validation Loss: {val_loss}\")\n",
    "\n",
    "\n",
    "train_model(model, train_dataloader, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z1-pRdHKKVKX",
    "outputId": "7f0ffd83-3fb7-4de9-ee6e-12cd13688e93"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(model, val_dataloader, device):\n",
    "    model.eval()  # Перемикаємо модель на режим оцінки\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Вимикаємо обчислення градієнтів\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Прямий прохід\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Обчислюємо Perplexity\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    perplexity = math.exp(avg_loss)  # Експоненціємо середні втрати\n",
    "    return perplexity\n",
    "\n",
    "# Приклад використання\n",
    "perplexity = calculate_perplexity(model, val_dataloader, device)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHxQZbr2KbXd",
    "outputId": "dea12cd1-9b3b-4dd6-d670-560e7836a7af"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu_score(model, dataloader, device):\n",
    "    model.eval()  # Перемикаємо модель на режим оцінки\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():  # Вимикаємо обчислення градієнтів\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Генерація тексту\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50)\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Додаємо еталонні та згенеровані тексти\n",
    "            reference_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            references.append([reference_text])  # Зберігаємо як список рядків\n",
    "            hypotheses.append(generated_text)  # Зберігаємо як рядок\n",
    "\n",
    "    # Обчислюємо BLEU score для всього набору даних\n",
    "    smoothing_function = SmoothingFunction().method4  # Можна змінити метод згладжування\n",
    "    bleu_score = sentence_bleu(references, hypotheses, smoothing_function=smoothing_function)\n",
    "    return bleu_score\n",
    "\n",
    "# Приклад використання\n",
    "bleu_score = calculate_bleu_score(model, val_dataloader, device)\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60rTc0-hMbEU",
    "outputId": "2073118c-1a40-4a03-c1ef-89e4237724c6"
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"як умру то пох\",\n",
    "    \"Зоре моя вечірняя, зійди над горою,\",\n",
    "    \"Чого мені тяжко, чого мені нудно, чого серце плаче, ридає, кричить,\",\n",
    "    \"Якось-то йдучи уночі понад Невою… та, йдучи, міркую сам-таки з собою: 'якби-то,— думаю,\",\n",
    "    \"Мені здається, я не знаю\",\n",
    "    \"У тієї Катерини\"\n",
    "]\n",
    "\n",
    "model.eval()  # Перемикаємо модель на режим оцінки\n",
    "with torch.no_grad():  # Вимикаємо обчислення градієнтів\n",
    "    for input_text in inputs:\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "        attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "        # Генерація тексту\n",
    "        outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=200, temperature=0.7, top_p=0.9, top_k=50)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Input: {input_text}\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
